# -*- coding: utf-8 -*-
"""Imbalancedd_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mrgMhWcFs-zvxsVhlqQ40zYMsBaui0MS
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

df=pd.read_csv("/content/train.csv")
print(df)

df.head()

df.shape

df.describe()

#check the null values
df.isnull().sum()

#check duplicate data
df.duplicated().sum()

# drop the duplicates
df.drop_duplicates()

df.dtypes

df.head(5)

target_count = df.target.value_counts()
print('Class 0:', target_count[0])
print('Class 1:', target_count[1])

print('Proportion of class 0 is ', round(target_count[0] * 100 / (target_count[1] + target_count[0]), 2),'%')

target_count.plot(kind='bar', title='Count (target)');

df['target'].isnull().sum()
df = df.dropna(subset=['target'])   # Remove rows where target is missing
# Drop rows where target is NaN
df = df.dropna(subset=['target'])

# If target is float, convert to int
df['target'] = df['target'].astype(int)

df = df.fillna(df.median())
df = df.fillna(df.mode().iloc[0])

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
X = df.drop("target", axis=1)
y = df["target"]
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

from imblearn.under_sampling import ClusterCentroids

#4.first apply random undersampling cluster centroid
cc = ClusterCentroids(random_state=42)
X_train_under, y_train_under = cc.fit_resample(X_train_scaled, y_train)

print("\nAfter Cluster Centroids Undersampling:")
print(y_train_under.value_counts())

import matplotlib.pyplot as plt
import seaborn as sns
# 1. Bar Plot for Class Distribution
plt.figure(figsize=(6,4))
sns.countplot(x=y_train_under)
plt.title("Class Distribution After Cluster Centroids Undersampling")
plt.xlabel("Class")
plt.ylabel("Count")
plt.show()

#5.apply ML models
#logistic Regression
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score
from sklearn.metrics import roc_curve

from sklearn.metrics import accuracy_score, f1_score, roc_auc_score
from sklearn.metrics import roc_curve

# 1. Train Logistic Regression
log_reg = LogisticRegression()
log_reg.fit(X_train_under, y_train_under)

# 2. Predictions
y_pred = log_reg.predict(X_test_scaled)

# For AUROC we need predicted probabilities
y_prob = log_reg.predict_proba(X_test_scaled)[:, 1]

# 3. Metrics
accuracy = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auroc = roc_auc_score(y_test, y_prob)

print("Logistic Regression Evaluation on Undersampled Data:")
print("Accuracy:", accuracy)
print("F1-Score:", f1)
print("AUROC:", auroc)

"""Report:- Here for the logistic regression after undersampling the Accuracy: 0.5510660700184259

F1-Score: 0.08869890462196099
AUROC: 0.6031000743316888
"""

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve

fpr, tpr, thresholds = roc_curve(y_test, y_prob)

plt.figure(figsize=(6,5))
plt.plot(fpr, tpr, label="ROC Curve")
plt.plot([0,1], [0,1], linestyle='--')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve - Logistic Regression")
plt.legend()
plt.show()

# IMPORTS
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score

# 1. Train KNN
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train_under, y_train_under)

# 2. Predictions
y_pred_knn = knn.predict(X_test_scaled)

# For AUROC
y_prob_knn = knn.predict_proba(X_test_scaled)[:, 1]

# 3. Metrics
accuracy_knn = accuracy_score(y_test, y_pred_knn)
f1_knn = f1_score(y_test, y_pred_knn)
auroc_knn = roc_auc_score(y_test, y_prob_knn)

print("KNN Evaluation on Undersampled Data:")
print("Accuracy:", accuracy_knn)
print("F1-Score:", f1_knn)
print("AUROC:", auroc_knn)

#ii)kNN model
#import the libraries
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, f1_score,roc_auc_score

# 1. Initialize KNN model
knn = KNeighborsClassifier(n_neighbors=5)

# 2. Train the KNN model on undersampled training data
knn.fit(X_train_under, y_train_under)
# 3. Predict on test data
y_pred_knn = knn.predict(X_test)
y_prob_knn = knn.predict_proba(X_test_scaled)[:, 1]
# 4. Calculate metrics
accuracy_knn = accuracy_score(y_test, y_pred_knn)
f1_knn = f1_score(y_test, y_pred_knn)
auroc_knn = roc_auc_score(y_test, y_prob_knn)

print("KNN Results After Cluster Centroids Undersampling:")
print("Accuracy:", accuracy_knn)
print("F1-Score:", f1_knn)
print("AUROC:", auroc_knn)

"""Report:- Here for the knn after undersampling the Accuracy: 0.03737825743616741

F1-Score: 0.07206292819081452
AUROC: 0.5137263284382257
"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score

# 1. Initialize the model
dt = DecisionTreeClassifier(random_state=42)

# 2. Fit the training data
dt.fit(X_train, y_train)
# 3. Predictions
y_pred_dt = dt.predict(X_test)

# 4. Predicted probabilities for AUROC
y_prob_dt = dt.predict_proba(X_test)[:, 1]

# 5. Metrics
accuracy_dt = accuracy_score(y_test, y_pred_dt)
f1_dt = f1_score(y_test, y_pred_dt)
auroc_dt = roc_auc_score(y_test, y_prob_dt)

# 6. Print results
print("Decision Tree Results:")
print("Accuracy:", accuracy_dt)
print("F1-Score:", f1_dt)
print("AUROC:", auroc_dt)

"""Result :- here for the Decision Tree Accuracy: 0.9180047380889708

F1-Score: 0.040061633281972264

AUROC: 0.49882388396553784

Random oversampling---> smote
"""

#again apply random oversampling --> smote
from imblearn.over_sampling import SMOTE

# Initialize SMOTE
sm = SMOTE(random_state=42)

# Apply SMOTE on the training data
X_train_over, y_train_over = sm.fit_resample(X_train_scaled, y_train)

print("After SMOTE Oversampling:")
print(y_train_over.value_counts())

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA

# 1. Class Distribution After SMOTE
plt.figure(figsize=(6,4))
sns.countplot(x=y_train_over)
plt.title("Class Distribution After SMOTE Oversampling")
plt.xlabel("Class")
plt.ylabel("Count")
plt.show()

#apply ML models --> log reg, knncls, DTCls
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score

# 1. Train Logistic Regression on SMOTE data
log_reg_smote = LogisticRegression()
log_reg_smote.fit(X_train_over, y_train_over)

# 2. Predictions on test data
y_pred_smote = log_reg_smote.predict(X_test_scaled)

# 3. Probabilities for AUROC
y_prob_smote = log_reg_smote.predict_proba(X_test_scaled)[:, 1]

# 4. Metrics
accuracy_smote = accuracy_score(y_test, y_pred_smote)
f1_smote = f1_score(y_test, y_pred_smote)
auroc_smote = roc_auc_score(y_test, y_prob_smote)

# 5. Print results
print("Logistic Regression Results After SMOTE:")
print("Accuracy:", accuracy_smote)
print("F1-Score:", f1_smote)
print("AUROC:", auroc_smote)

"""Result :- here Logistic Regression Results After SMOTE:

Accuracy: 0.5908133719399842

F1-Score: 0.09226277372262774

AUROC: 0.6072927859748043
"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score

# 1. Train KNN on SMOTE Oversampled Data
knn_smote = KNeighborsClassifier(n_neighbors=5)   # you can tune n_neighbors later
knn_smote.fit(X_train_over, y_train_over)

# 2. Predictions on test data
y_pred_knn_smote = knn_smote.predict(X_test_scaled)

# 3. Probabilities for AUROC
y_prob_knn_smote = knn_smote.predict_proba(X_test_scaled)[:, 1]

# 4. Metrics
accuracy_knn_smote = accuracy_score(y_test, y_pred_knn_smote)
f1_knn_smote = f1_score(y_test, y_pred_knn_smote)
auroc_knn_smote = roc_auc_score(y_test, y_prob_knn_smote)

# 5. Print results
print("KNN Results After SMOTE:")
print("Accuracy:", accuracy_knn_smote)
print("F1-Score:", f1_knn_smote)
print("AUROC:", auroc_knn_smote)

"""Result :- here KNN Results After SMOTE:

Accuracy: 0.55290866017373

F1-Score: 0.07965321051205636

AUROC: 0.5400069613744816
"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score

# 1. Train Decision Tree on SMOTE Oversampled Data
dt_smote = DecisionTreeClassifier(random_state=42)
dt_smote.fit(X_train_over, y_train_over)

# 2. Predictions on test data
y_pred_dt_smote = dt_smote.predict(X_test_scaled)

# 3. Probabilities for AUROC
y_prob_dt_smote = dt_smote.predict_proba(X_test_scaled)[:, 1]

# 4. Metrics
accuracy_dt_smote = accuracy_score(y_test, y_pred_dt_smote)
f1_dt_smote = f1_score(y_test, y_pred_dt_smote)
auroc_dt_smote = roc_auc_score(y_test, y_prob_dt_smote)

# 5. Print results
print("Decision Tree Results After SMOTE:")
print("Accuracy:", accuracy_dt_smote)
print("F1-Score:", f1_dt_smote)
print("AUROC:", auroc_dt_smote)

"""Report :- Here Decision Tree Results After SMOTE:

Accuracy: 0.9119505132929718

F1-Score: 0.03741007194244604

AUROC: 0.4956792298774875
"""

